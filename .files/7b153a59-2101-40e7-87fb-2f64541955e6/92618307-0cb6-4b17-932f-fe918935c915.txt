of activation functions to choose from. Some more appropriate for hidden layers. Others more appropriate for output layers. Activation functions are necessary because they make the layers productive. They separate features as well as scale the outputs. Now that we completed the hidden layer, let's look at the prediction on the output layer. So we are going to establish those three incoming values out of the hidden nodes and they're going to be propagated into the output node as the input variables. So we're going to substitute those in. We are then going to take the unique weights between the hidden and output node as substitute those as well. We will complete our addition and multiplication operation at our bias value unique to this node as well and that will be our raw output. We will then pass that to the sigmoid activation function. This will scale the value to be between 0 and 1, resembling a probability. Recall if our prediction is less than 0.5, it's light. If it's greater than