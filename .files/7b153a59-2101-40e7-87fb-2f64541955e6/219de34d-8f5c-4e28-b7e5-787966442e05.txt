 We are going to build a neural network that predicts a light or dark font for a given background color. That will be expressed in RGB values. That will be the input layer. Think of it as a single pixel in an image. The hidden layer will be three nodes containing the way in bias values. And then the output layer will contain the final prediction value. Each input layer node will have a relationship with each hidden layer node, establishing a weight between each one. There will also be a bias value associated with each hidden node. How many hidden layers and the number of nodes in each hidden layer is determined by the specific problem and experimentation. One or more nodes exist in the output layer. In this case, we only have one. That will hold the final prediction. Let's zoom in on that output node. If our prediction is less than 0.5, we will categorize it as light. If it is greater than or equal to 0.5, we will categorize it as dark. On top of these multiplication and addition operations in each node, we will also apply activation functions. Let's pass this greenish background color through our neural network. We will break this up into three RGB values, which we will then divide by 255, so they are linearly scaled between 0 and 1. We are then going to pass this color through the neural network to make a prediction of a light or dark font. We will pass those first three input values to the first hidden node, and those will substitute the x1, x2, x3 values. And then we will have those three weights we showed earlier. Those weights are fitted using stochastic gradient descent or other optimization techniques that can only be done with thousands of samples of pre-labeled background colors. That process will also calculate the bias value. Finally, we perform that multiplication and addition. We are not done though. We need to take that output and pass it through the RayLoo function. The RayLoo function simply takes negative values and turns them to 0. If they are positive, it just leaves the value alone. We repeat this process for every other hidden node. We pass forward the scaled RGB values for that given background color. Those are going to be multiplied with the unique respective weights for that given hidden node associated with each input node. We will also add that bias value, then pass it through the activation function. For this particular color, it may seem that RayLoo is not doing anything because no negative values are coming out of the hidden nodes. That's okay. There are other colors that would. End outputting a negative value would result in a 0. Regarding which activation function to use, RayLoo is common in hidden layers. There are a handful of activation functions to choose from. Some more appropriate for hidden layers. Others more appropriate for output layers. Activation functions are necessary because they make the layers productive. They separate features as well as scale the outputs. Now that we completed the hidden layer, let's look at the prediction on the output layer. So we are going to establish those three incoming values out of the hidden nodes and they're going to be propagated into the output node as the input variables. So we're going to substitute those in. We are then going to take the unique weights between the hidden and output node as substitute those as well. We will complete our addition and multiplication operation at our bias value unique to this node as well and that will be our raw output. We will then pass that to the sigmoid activation function. This will scale the value to be between 0 and 1, resembling a probability. Recall if our prediction is less than 0.5, it's light. If it's greater than equal to 0.5, it's dark. Well, because our value is 0.843, we will categorize it as dark. Thanks for watching. I hope you enjoyed this video. Chapter 7 of my book Essential Math for Data Science walks through how to create this neural network completely from scratch. This also includes how to solve the way in bias values using stochastic gradient descent and back propagation. Links to the books and animation source code are in the description. See you next time.